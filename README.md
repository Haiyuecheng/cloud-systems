# Multi-Cloud Project

This project is a multi-cloud application that includes both frontend and backend services. The frontend is built with a modern JavaScript framework, and the backend is built with Node.js. The application is deployed on AWS using EKS (Elastic Kubernetes Service) and ECR (Elastic Container Registry).

## Project Structure

## Prerequisites

- Node.js
- Docker
- kubectl
- eksctl
- AWS CLI

## Setup

# Streamlined Guide: Using Claude as AI Assistant to Terraform

## Step 1: Use Claude to Generate Terraform Code

1. Start a conversation with Claude.
2. Ask Claude to create Terraform code for an S3 bucket. Use a prompt like:
"Please provide Terraform code to create an S3 bucket in AWS with a unique name."
3. Claude should generate code similar to this:
provider "aws" {
  region = "us-west-2"  # Replace with your desired region
}

resource "random_id" "bucket_suffix" {
  byte_length = 8
}

resource "aws_s3_bucket" "my_bucket" {
  bucket = "my-unique-bucket-name-${random_id.bucket_suffix.hex}"

  tags = {
    Name        = "My bucket"
    Environment = "Dev"
  }
}

resource "aws_s3_bucket_acl" "my_bucket_acl" {
  bucket = aws_s3_bucket.my_bucket.id
  acl    = "private"
}
4. Save this code for use in Step 5.

## Step 2: Create IAM Role for EC2

1. Log in to the AWS Management Console.
2. Navigate to the IAM dashboard.
3. Click "Roles" in the left sidebar, then "Create role".
4. Choose "AWS service" as the trusted entity type and "EC2" as the use case.
5. Search for and attach the "AdministratorAccess" policy.
Note: In a production environment, use a more restricted policy.
6. Name the role "EC2Admin" and provide a description.
7. Review and create the role.

## Step 3: Launch EC2 Instance

1. Go to the EC2 dashboard in the AWS Management Console.
2. Click "Launch Instance".
3. Choose an Amazon Linux 2 AMI.
4. Select a t2.micro instance type.
5. Configure instance details:
    - Network: Default VPC
    - Subnet: Any available
    - Auto-assign Public IP: Enable
    - IAM role: Select "EC2Admin"
6. Keep default storage settings.
7. Add a tag: Key="Name", Value="workstation".
8. Create a security group allowing SSH access from EC2 Connect IP.
9. Review and launch, selecting or creating a key pair.

## Step 4: Connect to EC2 Instance and Install Terraform

1. From the EC2 dashboard, select your "workstation" instance.
2. Click "Connect" and use the "EC2 Instance Connect" method.
3. In the browser-based SSH session, update system packages:
 ```
sudo yum update -y

```

4. Install yum-utils:
    
    ```
    sudo yum install -y yum-utils
    
    ```
    
5. Add HashiCorp repository:
    
    ```
    sudo yum-config-manager --add-repo https://rpm.releases.hashicorp.com/AmazonLinux/hashicorp.repo
    
    ```
    
6. Install Terraform:
    
    ```
    sudo yum -y install terraform
    
    ```
    
7. Verify installation:
    
    ```
    terraform version
    
    ```
    ## Step 5: Apply Terraform Configuration

1. Create a new directory and navigate to it:
    
    ```
    mkdir terraform-project && cd terraform-project
    
    ```
    
2. Create and open [main.tf](http://main.tf/):
    
    ```
    nano main.tf
    
    ```
    
3. Paste the Terraform code generated by Claude in Step 1.
4. Save and exit the editor (in nano, press Ctrl+X, then Y, then Enter).
5. Initialize Terraform:
    
    ```
    terraform init
    
    ```
    
6. Review the plan:
terraform plan

​
Apply the configuration:
terraform apply

​
Type "yes" when prompted to create the resources.
Step 6: Verify S3 Bucket Creation
Use AWS CLI to list buckets:
aws s3 ls

​
Verify that your new bucket is in the list.
Step 7: Create the Cloud DynamoDB tables
provider "aws" {
  region = "us-east-1"  # change to your own region
}

# Tables DynamoDB
resource "aws_dynamodb_table" "cloudmart_products" {
  name           = "cloudmart-products"
  billing_mode   = "PAY_PER_REQUEST"
  hash_key       = "id"

  attribute {
    name = "id"
    type = "S"
  }
}

resource "aws_dynamodb_table" "cloudmart_orders" {
  name           = "cloudmart-orders"
  billing_mode   = "PAY_PER_REQUEST"
  hash_key       = "id"

  attribute {
    name = "id"
    type = "S"
  }
}

resource "aws_dynamodb_table" "cloudmart_tickets" {
  name           = "cloudmart-tickets"
  billing_mode   = "PAY_PER_REQUEST"
  hash_key       = "id"

  attribute {
    name = "id"
    type = "S"
  }
}

1. Apply the configuration:
    
    ```
    terraform apply
    
    ```
    
2. Type "yes" when prompted to create the resources.


Part 1 - Docker
Step 1: Install Docker on EC2
Execute the following commands:
sudo yum update -y
sudo yum install docker -y
sudo systemctl start docker
sudo docker run hello-world
sudo systemctl enable docker
docker --version
sudo usermod -a -G docker $(whoami)
newgrp docker
​
sudo usermod -a -G docker $(whoami)
newgrp docker

Step 2: Create Docker image for CloudMart
Backend
Create folder and download source code:
mkdir -p challenge-day2/backend && cd challenge-day2/backend
wget https://tcb-public-events.s3.amazonaws.com/mdac/resources/day2/cloudmart-backend.zip
unzip cloudmart-backend.zip
​
Create .env file:
nano .env
​
Content of .env:
PORT=5000
AWS_REGION=# your region
BEDROCK_AGENT_ID=<your-bedrock-agent-id>
BEDROCK_AGENT_ALIAS_ID=<your-bedrock-agent-alias-id>
OPENAI_API_KEY=<your-openai-api-key>
OPENAI_ASSISTANT_ID=<your-openai-assistant-id>

Create Dockerfile:
nano Dockerfile
​
Content of Dockerfile:
FROM node:18
WORKDIR /usr/src/app
COPY package*.json ./
RUN npm install
COPY . .
EXPOSE 5000
CMD ["npm", "start"]
​
Frontend
Create folder and download source code:
cd ..
mkdir frontend && cd frontend
wget https://tcb-public-events.s3.amazonaws.com/mdac/resources/day2/cloudmart-frontend.zip
unzip cloudmart-frontend.zip

Create Dockerfile:
nano Dockerfile
​
Content of Dockerfile:
FROM node:16-alpine as build
WORKDIR /app
COPY package*.json ./
RUN npm ci
COPY . .
RUN npm run build

FROM node:16-alpine
WORKDIR /app
RUN npm install -g serve
COPY --from=build /app/dist /app
ENV PORT=5001
ENV NODE_ENV=production
EXPOSE 5001
CMD ["serve", "-s", ".", "-l", "5001"]

Cluster Setup on AWS Elastic Kubernetes Services (EKS)
Create a user named eksuser with Admin privileges and authenticate with it
aws configure
​
Install the CLI tool eksctl
curl --silent --location "https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz" | tar xz -C /tmp
sudo cp /tmp/eksctl /usr/bin
eksctl version
​
Install the CLI tool kubectl
curl -o kubectl https://amazon-eks.s3.us-west-2.amazonaws.com/1.18.9/2020-11-02/bin/linux/amd64/kubectl
chmod +x ./kubectl
mkdir -p $HOME/bin && cp ./kubectl $HOME/bin/kubectl && export PATH=$PATH:$HOME/bin
echo 'export PATH=$PATH:$HOME/bin' >> ~/.bashrc
kubectl version --short --client

4. Create an EKS Cluster
    
    ```bash
    eksctl create cluster \
      --name cloudmart \
      --region # your region \
      --nodegroup-name standard-workers \
      --node-type t3.medium \
      --nodes 1 \
      --with-oidc \
      --managed
    ```
    
5. Connect to the EKS cluster using the `kubectl` configuration
    
    ```bash
    aws eks update-kubeconfig --name cloudmart
    ```
    
6. Verify Cluster Connectivity
    
    ```bash
    kubectl get svc
    kubectl get nodes
    ```
    
7. Create a Role & Service Account to provide pods access to services used by the application (DynamoDB, Bedrock, etc).

eksctl create iamserviceaccount \
  --cluster=cloudmart \
  --name=cloudmart-pod-execution-role \
  --role-name CloudMartPodExecutionRole \
  --attach-policy-arn=arn:aws:iam::aws:policy/AdministratorAccess\
  --region us-east-1 \
  --approve
​
NOTE: In this project, Admin privileges were used to facilitate educational purposes. Always remember to follow the principle of least privilege in production environments


Backend Deployment on Kubernetes
Create an ECR Repository for the Backend and upload the Docker image to it
Repository name: cloudmart-backend
​
Switch to backend folder
cd ../..
cd challenge-day2/backend
​
Follow the ECR steps to build your Docker image
Create a Kubernetes deployment file (YAML) for the Backend
cd ../..
cd challenge-day2/backend
nano cloudmart-backend.yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: cloudmart-backend-app
spec:
  replicas: 1
  selector:
    matchLabels:
      app: cloudmart-backend-app
  template:
    metadata:
      labels:
        app: cloudmart-backend-app
    spec:
      serviceAccountName: cloudmart-pod-execution-role
      containers:
      - name: cloudmart-backend-app
        image: public.ecr.aws/l4c0j8h9/cloudmart-backend:latest
        env:
        - name: PORT
          value: "5000"
        - name: AWS_REGION
          value: # your region ""
        - name: BEDROCK_AGENT_ID
          value: "xxxxxx"
        - name: BEDROCK_AGENT_ALIAS_ID
          value: "xxxx"
        - name: OPENAI_API_KEY
          value: "xxxxxx"
        - name: OPENAI_ASSISTANT_ID
          value: "xxxx"
---

apiVersion: v1
kind: Service
metadata:
  name: cloudmart-backend-app-service
spec:
  type: LoadBalancer
  selector:
    app: cloudmart-backend-app
  ports:
    - protocol: TCP
      port: 5000
      targetPort: 5000

### **Deploy the Backend on Kubernetes**

```yaml
kubectl apply -f cloudmart-backend.yaml
```

Monitor the status of objects being created and obtain the public IP generated for the API

```yaml
kubectl get pods
kubectl get deployment
kubectl get service
```

## Frontend Deployment on Kubernetes

### Preparation

Change the Frontend's .env file to point to the API URL created within Kubernetes obtained by the `kubectl get service` command

cd ../challenge-day2/frontend
nano .env
​
Content of .env:
VITE_API_BASE_URL=http://<your_url_kubernetes_api>:5000/api

​
Create an ECR Repository for the Frontend and upload the Docker image to it
Repository name: cloudmart-frontend
​
Follow the ECR steps to build your Docker image
Create a Kubernetes deployment file (YAML) for the Frontend

nano cloudmart-frontend.yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: cloudmart-frontend-app
spec:
  replicas: 1
  selector:
    matchLabels:
      app: cloudmart-frontend-app
  template:
    metadata:
      labels:
        app: cloudmart-frontend-app
    spec:
      serviceAccountName: cloudmart-pod-execution-role
      containers:
      - name: cloudmart-frontend-app
        image: public.ecr.aws/l4c0j8h9/cloudmart-frontend:latest
---

apiVersion: v1
kind: Service
metadata:
  name: cloudmart-frontend-app-service
spec:
  type: LoadBalancer
  selector:
    app: cloudmart-frontend-app
  ports:
    - protocol: TCP
      port: 5001
      targetPort: 5001

Deploy the Frontend on Kubernetes
kubectl apply -f cloudmart-frontend.yaml
​
Monitor the status of objects being created and obtain the public IP generated for the API
kubectl get pods
kubectl get deployment
kubectl get service


At the end of the project, delete all resources:
kubectl delete service cloudmart-frontend-app-service
kubectl delete deployment cloudmart-frontend-app
kubectl delete service cloudmart-backend-app-service
kubectl delete deployment cloudmart-backend-app

eksctl delete cluster --name cloudmart --region # your region
````


Part 1: CI/CD Pipeline Configuration
Create a free account on GitHub and then create a new repository on GitHub called cloudmart
cd challenge-day2/frontend
<Run GitHub steps>
​
Start by pushing the changes in the CloudMart application source code to GitHub
git status
git add -A
git commit -m "app sent to repo"
git push

### **Configure AWS CodePipeline**

1. **Create a New Pipeline:**
    - Access AWS CodePipeline.
    - Start the 'Create pipeline' process.
    - Name: `cloudmart-cicd-pipeline`
    - Use the GitHub repository `cloudmart-application` as the source.
    - Add the 'cloudmartBuild' project as the build stage.
    - Add the 'cloudmartDeploy' project as the deployment stage.

### Configure **AWS CodeBuild to Build the Docker Image**

1. **Create a Build Project:**
    - Give the project a name (for example, **`cloudmartBuild`**).
    - Connect it to your existing GitHub repository (**`cloudmart-application`**).
    - **Image: amazonlinux2-x86_64-standard:4.0**
    - Configure the environment to support Docker builds. Enable "Enable this flag if you want to build Docker images or want your builds to get elevated privileges"
    - Add the environment variable **ECR_REPO** with the ECR repository URI.
    - For the build specification, use the following **`buildspec.yml`**:

    version: 0.2
phases:
  install:
    runtime-versions:
      docker: 20
  pre_build:
    commands:
      - echo Logging in to Amazon ECR...
      - aws --version
      - REPOSITORY_URI=$ECR_REPO
      - aws ecr-public get-login-password --region us-east-1 | docker login --username AWS --password-stdin public.ecr.aws/l4c0j8h9
  build:
    commands:
      - echo Build started on `date`
      - echo Building the Docker image...
      - docker build -t $REPOSITORY_URI:latest .
      - docker tag $REPOSITORY_URI:latest $REPOSITORY_URI:$CODEBUILD_RESOLVED_SOURCE_VERSION
  post_build:
    commands:
      - echo Build completed on `date`
      - echo Pushing the Docker image...
      - docker push $REPOSITORY_URI:latest
      - docker push $REPOSITORY_URI:$CODEBUILD_RESOLVED_SOURCE_VERSION
      - export imageTag=$CODEBUILD_RESOLVED_SOURCE_VERSION
      - printf '[{\"name\":\"cloudmart-app\",\"imageUri\":\"%s\"}]' $REPOSITORY_URI:$imageTag > imagedefinitions.json
      - cat imagedefinitions.json
      - ls -l

env:
  exported-variables: ["imageTag"]

artifacts:
  files:
    - imagedefinitions.json
    - cloudmart-frontend.yaml

1. **Add the AmazonElasticContainerRegistryPublicFullAccess permission to ECR in the service role**
- Access the IAM console > Roles.
- Look for the role created "cloudmartBuild" for CodeBuild.
- Add the permission **AmazonElasticContainerRegistryPublicFullAccess**.

### Configure AWS CodeBuild for Application Deployment

**Create a Deployment Project:**

- Repeat the process of creating projects in CodeBuild.
- Give this project a different name (for example, **`cloudmartDeployToProduction`**).
- Configure the environment variables AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY for the credentials of the user **`eks-user`** in Cloud Build, so it can authenticate to the Kubernetes cluster.

*Note: in a real-world production environment, it is recommended to use an IAM role for this purpose. In this practical exercise, we are directly using the credentials of the* **`eks-user`** *to facilitate the process, since our focus is on CI/CD and not on user authentication at this moment. The configuration of this process in EKS is more extensive. Refer to the Reference section and check "Enabling IAM principal access to your cluster"*

- For the deployment specification, use the following **`buildspec.yml`**:

version: 0.2

phases:
  install:
    runtime-versions:
      docker: 20
    commands:
      - curl -o kubectl https://amazon-eks.s3.us-west-2.amazonaws.com/1.18.9/2020-11-02/bin/linux/amd64/kubectl
      - chmod +x ./kubectl
      - mv ./kubectl /usr/local/bin
      - kubectl version --short --client
  post_build:
    commands:
      - aws eks update-kubeconfig --region # your region  --name cloudmart
      - kubectl get nodes
      - ls
      - IMAGE_URI=$(jq -r '.[0].imageUri' imagedefinitions.json)
      - echo $IMAGE_URI
      - sed -i "s|CONTAINER_IMAGE|$IMAGE_URI|g" cloudmart-frontend.yaml
      - kubectl apply -f cloudmart-frontend.yaml

Replace the image URI on line 18 of the cloudmart-frontend.yaml files with CONTAINER_IMAGE.
Commit and push the changes.
git add -A
git commit -m "replaced image uri with CONTAINER_IMAGE"
git push
​
Part 2: Test your CI/CD Pipeline
Make a Change on GitHub:
Update the application code in the cloudmart-application repository.
File src/components/MainPage/index.jsx line 93
Commit and push the changes.
git add -A
git commit -m "changed to Featured Products on CloudMart"
git push

2. **Observe the Pipeline Execution:**
    - Watch how CodePipeline automatically triggers the build.
    - After the build, the deployment phase should begin.
3. **Verify the Deployment:**
    - Check Kubernetes using **`kubectl`** commands to confirm the application update.
